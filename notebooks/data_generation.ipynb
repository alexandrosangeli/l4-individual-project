{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1hqVXZb21i1"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arq5hja5XUzc",
        "outputId": "2725fb42-fddd-4585-f10a-16bcc2887c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.4.2\n",
            "  Downloading pandas-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.4.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.2) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "Successfully installed pandas-1.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sequence-models\n",
            "  Downloading sequence_models-1.6.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sequence-models\n",
            "Successfully installed sequence-models-1.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas==1.4.2\n",
        "!pip install sequence-models\n",
        "!pip install biopython\n",
        "!pip install fair-esm\n",
        "\n",
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "from sequence_models.pretrained import load_model_and_alphabet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "APGYdhMXWeJy"
      },
      "outputs": [],
      "source": [
        "def update_dict(data, code, chain, domain, boundaries):\n",
        "    data[code] = data.get(code, {})\n",
        "    if chain not in data[code].keys():\n",
        "        data[code][chain] = {}\n",
        "    data[code][chain][str(int(domain))] = boundaries\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_cath(lines_range = None, file=\"../data/cath/cath_domain_boundaries.txt\", verb=False):\n",
        "    with open(file, \"r\") as f:\n",
        "        lines = f.readlines() if lines_range is None else f.readlines()[lines_range[0]:lines_range[1]]\n",
        "        lines = map(lambda x : x.strip(), lines)\n",
        "        big = 0\n",
        "        data = {}\n",
        "        for line in lines:\n",
        "            name, boundaries = line.split(\"\\t\")\n",
        "            code = name[:4]\n",
        "            chain = name[4]\n",
        "            domain = name[5:]\n",
        "            data = update_dict(data, code, chain, domain, boundaries)\n",
        "    # verb and pprint(data)\n",
        "    return data\n",
        "\n",
        "def boundaries(len_seq, domain, discontinuity_delimiter=','):\n",
        "    \"\"\"\n",
        "        Defines a boundary as the beginning of a domain ONLY in multi-domain proteins\n",
        "    \"\"\"\n",
        "    first_start = np.inf\n",
        "    bounds = np.zeros((len_seq), dtype=np.int8)\n",
        "    for k, v in domain.items():\n",
        "        boundary_positions = v.split(discontinuity_delimiter)\n",
        "        for b in boundary_positions:\n",
        "            start, end = [int(i) for i in b.split('-')]\n",
        "            if start < first_start:\n",
        "                first_start = start\n",
        "            bounds[start-1] = 1\n",
        "    bounds[first_start-1] = 0            \n",
        "    return np.array(bounds, dtype=np.bool_)\n",
        "\n",
        "\n",
        "cath = get_cath()\n",
        "\n",
        "\n",
        "with open('../data/cath/iid/chains_to_seq_iid.json') as json_file:\n",
        "    key_to_seq_dict = json.load(json_file)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQQtk0rn85zj"
      },
      "source": [
        "# CASP13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF8qIb9K-m9n",
        "outputId": "0c2adf7b-a363-4300-930f-19c21c5d5b55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/esm/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D-contact-regression.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written 10/80 entries\n",
            "Written 20/80 entries\n",
            "Written 30/80 entries\n",
            "Written 40/80 entries\n",
            "Written 50/80 entries\n",
            "Written 60/80 entries\n",
            "Written 70/80 entries\n",
            "Written 80/80 entries\n",
            "Time to train: 5.114464521408081\n",
            "Written 80/80 entries\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['in', 'out', 'seq_len', 'key'])\n",
        "df['in'] = df['in'].astype(object)\n",
        "df['out'] = df['out'].astype(object)\n",
        "\n",
        "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t12_35M_UR50D\")\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "counter = 0\n",
        "\n",
        "\n",
        "with open('data/data_generation/casp13_data.json') as json_file:\n",
        "    casp13 = json.load(json_file)\n",
        "\n",
        "total = len(casp13['domains'])\n",
        "\n",
        "start = time.time()\n",
        "for name, domain in casp13['domains'].items():\n",
        "    seq = casp13['seqs'][name]\n",
        "    data = [[f\"p{counter}\", seq]]\n",
        "\n",
        "    _, _, batch_tokens = batch_converter(data)\n",
        "    batch_tokens = batch_tokens.to(device)\n",
        "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "    # Extract per-residue representations (on CPU)\n",
        "    with torch.no_grad():\n",
        "        results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
        "\n",
        "    input = results[\"representations\"][12][0].cpu().detach().numpy().astype(np.float16)\n",
        "    target = boundaries(len(seq), domain, ';')\n",
        "\n",
        "    new_df = pd.DataFrame({'in':[input], 'out':[target], 'seq_len':[len(seq)], 'key':name})\n",
        "    df = pd.concat([df,new_df], ignore_index=True)\n",
        "    counter += 1\n",
        "    if counter % 10 == 0:\n",
        "        print(f'Written {counter}/{total} entries')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Time to train: {end - start}\")\n",
        "\n",
        "print(f'Written {counter}/{total} entries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyhRRyB2AfNS"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('casp13_test_model.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy-3jjEDJ63i"
      },
      "source": [
        "# Carp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGc4yIckOgI1",
        "outputId": "53d0c899-e522-4e91-b5e8-a1b36caf3c6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CARP(\n",
              "  (model): ByteNetLM(\n",
              "    (embedder): ByteNet(\n",
              "      (embedder): Embedding(30, 8, padding_idx=28)\n",
              "      (up_embedder): PositionFeedForward(\n",
              "        (conv): Conv1d(8, 1024, kernel_size=(1,), stride=(1,))\n",
              "      )\n",
              "      (layers): ModuleList(\n",
              "        (0): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(128,), dilation=(64,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(256,), dilation=(128,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (8): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (9): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (10): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (11): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (12): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (13): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (14): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(128,), dilation=(64,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (15): ByteNetBlock(\n",
              "          (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(256,), dilation=(128,))\n",
              "          (sequence1): Sequential(\n",
              "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (4): GELU(approximate='none')\n",
              "          )\n",
              "          (sequence2): Sequential(\n",
              "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): PositionFeedForward(\n",
              "              (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (decoder): PositionFeedForward(\n",
              "      (conv): Conv1d(1024, 30, kernel_size=(1,), stride=(1,))\n",
              "    )\n",
              "    (last_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model, collater = load_model_and_alphabet('data/data_generation/carp_38M.pt')\n",
        "model = model.to(device)\n",
        "model.eval() # disable dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2tJ9ezYQFiP",
        "outputId": "72f8a818-77f2-4a68-e44a-5b2dcb790a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written 1000/8497 entries\n",
            "Written 2000/8497 entries\n",
            "Written 3000/8497 entries\n",
            "Written 4000/8497 entries\n",
            "Written 5000/8497 entries\n",
            "Written 6000/8497 entries\n",
            "Written 7000/8497 entries\n",
            "Written 8000/8497 entries\n",
            "Time to train: 168.65171456336975\n",
            "Written 8497/8497 entries\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['in', 'out', 'seq_len', 'key'])\n",
        "df['in'] = df['in'].astype(object)\n",
        "df['out'] = df['out'].astype(object)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "start = time.time()\n",
        "for chain, seq in key_to_seq_dict.items():\n",
        "\n",
        "    pdb_code = chain[:4]\n",
        "\n",
        "    seqs = [[seq]]\n",
        "    x = collater(seqs)[0] # (n, max_len)\n",
        "    x = x.to(device)\n",
        "    rep = model(x)  # (n, max_len, d_model)\n",
        "    input = rep['representations'][16][0].cpu().detach().numpy().astype(np.float16)\n",
        "    domain = cath[pdb_code][chain[-1]]\n",
        "    target = boundaries(len(seq), domain)\n",
        "\n",
        "    new_df = pd.DataFrame({'in':[input], 'out':[target], 'seq_len':[len(seq)], 'key':chain})\n",
        "    df = pd.concat([df,new_df], ignore_index=True)\n",
        "    counter += 1\n",
        "    if counter % 1000 == 0:\n",
        "        print(f'Written {counter}/{len(key_to_seq_dict)} entries')\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Time to train: {end - start}\")\n",
        "print(f'Written {counter}/{len(key_to_seq_dict)} entries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC-ZSAT7uCmX"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('carp38M_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GruhDd2KCcM"
      },
      "source": [
        "# ESM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aAz1ZpWKrIX",
        "outputId": "741d22cc-2673-4dd1-8a9e-8d66f9d3de43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_esm_main\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ESM2(\n",
              "  (embed_tokens): Embedding(33, 480, padding_idx=1)\n",
              "  (layers): ModuleList(\n",
              "    (0): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (8): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (9): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (10): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (11): TransformerLayer(\n",
              "      (self_attn): MultiheadAttention(\n",
              "        (k_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (v_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (q_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (out_proj): Linear(in_features=480, out_features=480, bias=True)\n",
              "        (rot_emb): RotaryEmbedding()\n",
              "      )\n",
              "      (self_attn_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "      (fc1): Linear(in_features=480, out_features=1920, bias=True)\n",
              "      (fc2): Linear(in_features=1920, out_features=480, bias=True)\n",
              "      (final_layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (contact_head): ContactPredictionHead(\n",
              "    (regression): Linear(in_features=240, out_features=1, bias=True)\n",
              "    (activation): Sigmoid()\n",
              "  )\n",
              "  (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=480, out_features=480, bias=True)\n",
              "    (layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model, alphabet = torch.hub.load(\"facebookresearch/esm:main\", \"esm2_t12_35M_UR50D\")\n",
        "\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()  # disables dropout for deterministic results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28rLj1-9KHi6",
        "outputId": "d0f63292-b2d7-42c3-97de-db4006038ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written 1000/8497 entries\n",
            "Written 2000/8497 entries\n",
            "Written 3000/8497 entries\n",
            "Written 4000/8497 entries\n",
            "Written 5000/8497 entries\n",
            "Written 6000/8497 entries\n",
            "Written 7000/8497 entries\n",
            "Written 8000/8497 entries\n",
            "Time to train: 203.44633269309998\n",
            "Written 8497/8497 entries\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['in', 'out', 'seq_len', 'key'])\n",
        "df['in'] = df['in'].astype(object)\n",
        "df['out'] = df['out'].astype(object)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for chain, seq in key_to_seq_dict.items():\n",
        "    pdb_code = chain[:4]\n",
        "\n",
        "    data = [[f\"p{counter}\", seq]]\n",
        "\n",
        "    _, _, batch_tokens = batch_converter(data)\n",
        "    batch_tokens = batch_tokens.to(device)\n",
        "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "    # Extract per-residue representations (on CPU)\n",
        "    with torch.no_grad():\n",
        "        results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
        "\n",
        "    input = results[\"representations\"][12][0].cpu().detach().numpy().astype(np.float16)\n",
        "    domain = cath[pdb_code][chain[-1]]\n",
        "    target = boundaries(len(seq), domain)\n",
        "\n",
        "    new_df = pd.DataFrame({'in':[input], 'out':[target], 'seq_len':[len(seq)], 'key':chain})\n",
        "    df = pd.concat([df,new_df], ignore_index=True)\n",
        "    counter += 1\n",
        "    if counter % 1000 == 0:\n",
        "        print(f'Written {counter}/{len(key_to_seq_dict)} entries')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Time to train: {end - start}\")\n",
        "print(f'Written {counter}/{len(key_to_seq_dict)} entries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5y9mo0WNCXd"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('esm35M_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13HPfi2mgfU"
      },
      "source": [
        "# One hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFxH8-ApmhVI"
      },
      "outputs": [],
      "source": [
        "def unique_amino_acids():\n",
        "    amino_acids = set()\n",
        "    for seq in key_to_seq_dict.values():\n",
        "        for aa in seq:\n",
        "            amino_acids.add(aa)\n",
        "    return str(sorted(list(amino_acids)))\n",
        "\n",
        "\n",
        "def one_hot_seq(seq):\n",
        "    amino_acids = \"\".join(list({'S', 'K', 'W', 'V', 'A', 'P', 'M', 'Q', 'I', 'H', 'Y', 'G', 'D', 'R', 'N', 'C', 'E', 'L', 'F', 'X', 'T'}))\n",
        "    encoded = np.array([1 if elt == 'A' else 0 for elt in seq],dtype=np.float64)\n",
        "    # start from the second element since the first one is A and was created above\n",
        "    for amino_acid in amino_acids[1:]:\n",
        "        new = np.array([1 if elt == amino_acid else 0 for elt in seq])\n",
        "        encoded = np.vstack((encoded, new))\n",
        "    return encoded.astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twSrerHJmm40",
        "outputId": "6abdab2a-d47a-4fc1-e492-61abff6a496d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Written 1000/8497 entries\n",
            "Written 2000/8497 entries\n",
            "Written 3000/8497 entries\n",
            "Written 4000/8497 entries\n",
            "Written 5000/8497 entries\n",
            "Written 6000/8497 entries\n",
            "Written 7000/8497 entries\n",
            "Written 8000/8497 entries\n",
            "Written 8497/8497 entries\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(columns=['in', 'out', 'seq_len', 'key'])\n",
        "df['in'] = df['in'].astype(object)\n",
        "df['out'] = df['out'].astype(object)\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for chain, seq in key_to_seq_dict.items():\n",
        "    pdb_code = chain[:4]\n",
        "    input = one_hot_seq(seq).T\n",
        "    domain = cath[pdb_code][chain[-1]]\n",
        "    target = boundaries(len(seq), domain)\n",
        "\n",
        "    new_df = pd.DataFrame({'in':[input], 'out':[target], 'seq_len':[len(seq)], 'key':chain})\n",
        "    df = pd.concat([df,new_df], ignore_index=True)\n",
        "    counter += 1\n",
        "    if counter % 1000 == 0:\n",
        "        print(f'Written {counter}/{len(key_to_seq_dict)} entries')\n",
        "\n",
        "print(f'Written {counter}/{len(key_to_seq_dict)} entries')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj9DzUcsm-0m"
      },
      "outputs": [],
      "source": [
        "df.to_pickle('onehot_data.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
