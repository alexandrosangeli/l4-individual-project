{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from helpers.helper import get_cath\n",
    "\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "\n",
    "import requests\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import shutil\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from pprint import pprint\n",
    "\n",
    "cath = get_cath()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the reverse mapping\n",
    "with open('../data/sword2/SWORD2/misc/new_iid/pdb_to_uniprot_map.json') as json_file:\n",
    "    pdb_to_uni_map = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_to_af_map = {}\n",
    "for elt in pdb_to_uni_map['results']:\n",
    "    pdb = elt['from']\n",
    "    databases = [db['database'] for db in elt['to']['uniProtKBCrossReferences']]\n",
    "    for i, db in enumerate(databases):\n",
    "        if db == 'AlphaFoldDB':\n",
    "            af_id = (elt['to']['uniProtKBCrossReferences'][i]['id'])\n",
    "            if pdb_to_af_map.get(pdb):\n",
    "                pdb_to_af_map[pdb].append(af_id)\n",
    "            else:\n",
    "                pdb_to_af_map[pdb] = [af_id]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10005\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for k, v in pdb_to_af_map.items():\n",
    "\ttotal += len(v)\n",
    "print(\"Total:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250/10005]\n",
      "[500/10005]\n",
      "[750/10005]\n",
      "[1000/10005]\n",
      "[1250/10005]\n",
      "[1500/10005]\n",
      "[1750/10005]\n",
      "[2000/10005]\n",
      "[2250/10005]\n",
      "[2500/10005]\n",
      "[2750/10005]\n",
      "[3000/10005]\n",
      "[3250/10005]\n",
      "[3500/10005]\n",
      "[3750/10005]\n",
      "[4000/10005]\n",
      "[4250/10005]\n",
      "[4500/10005]\n",
      "[4750/10005]\n",
      "[5000/10005]\n",
      "[5250/10005]\n",
      "[5500/10005]\n",
      "[5750/10005]\n",
      "[6000/10005]\n",
      "[6250/10005]\n",
      "[6500/10005]\n",
      "[6750/10005]\n",
      "[7000/10005]\n",
      "[7250/10005]\n",
      "[7500/10005]\n",
      "[7750/10005]\n",
      "[8000/10005]\n",
      "[8250/10005]\n",
      "[8500/10005]\n",
      "[8750/10005]\n",
      "[9000/10005]\n",
      "[9250/10005]\n",
      "[9500/10005]\n",
      "[9750/10005]\n",
      "[10000/10005]\n",
      "[10005/10005]\n"
     ]
    }
   ],
   "source": [
    "# download AF PDB files\n",
    "counter = 0\n",
    "for _, codes in pdb_to_af_map.items():\n",
    "\tfor code in codes:\n",
    "\t\t(bool, msg) = download_af_model(code)\n",
    "\t\tif not bool:\n",
    "\t\t\tprint(msg)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter % 250 == 0:\n",
    "\t\t\tprint(f\"[{counter}/{total}]\")\n",
    "print(f\"[{counter}/{total}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cath/iid/chains_to_seq_iid.json') as json_file:\n",
    "    chain_to_seq_iid = json.load(json_file)\n",
    "\n",
    "valid_pairs = []\n",
    "for chain, seq in chain_to_seq_iid.items():\n",
    "    if pdb_to_af_map.get(chain[:4]):\n",
    "        pdb_filename = f\"../data/pdb/new_iid/{chain[:4]}.pdb\"\n",
    "        chain_id = chain[-1]\n",
    "        with open(pdb_filename, \"r\") as pdb_file:\n",
    "            pdb_chains = {record.id: record.seq for record in SeqIO.parse(pdb_file, 'pdb-seqres')}\n",
    "            for key in pdb_chains.keys():\n",
    "                if key[-1] == chain[-1]:\n",
    "                    pdb_seq = pdb_chains[key]\n",
    "        af_seqs = []\n",
    "        for uniprot in pdb_to_af_map.get(chain[:4]):\n",
    "            af_seq = get_af_chain(uniprot, chain[-1])\n",
    "            if af_seq:\n",
    "                af_seqs.append((uniprot, af_seq))\n",
    "        for (uniprot, af_seq) in af_seqs:\n",
    "            if len(af_seq) == len(pdb_seq):\n",
    "                sim = sequence_sim(af_seq, pdb_seq)\n",
    "                if sim == 1.0:\n",
    "                    valid_pairs.append((chain, uniprot + f':{chain[-1]}'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (a, b) in valid_pairs:\n",
    "    if b[-1] != 'A':\n",
    "        print(a,b)\n",
    "        \n",
    "# These ARE ALL A chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1p4x:A 2\n",
      "1s7o:A 2\n"
     ]
    }
   ],
   "source": [
    "valid_pdbs = [x[0] for x in valid_pairs]\n",
    "valid_afs = [x[1] for x in valid_pairs]\n",
    "\n",
    "from collections import Counter\n",
    "for k,v in Counter(valid_pdbs).items():\n",
    "    if v > 1:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the pairs to their respective files\n",
    "with open('../data/sword2/SWORD2/misc/new_iid/sword_af.txt', \"w\") as af_f:\n",
    "    with open('../data/sword2/SWORD2/misc/new_iid/sword_pdb.txt', \"w\") as pdb_f:\n",
    "        for af in set(valid_afs):\n",
    "            af_f.write(af[:-2] + '\\n')\n",
    "        for pdb in set(valid_pdbs):\n",
    "            pdb_f.write(pdb[:-2] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a true_random_sample_keys.txt\n",
    "with open('../data/cath/iid/true_random_sample_keys.txt', 'w') as f:\n",
    "    for chain in chain_to_seq_iid.keys():\n",
    "        f.write(chain + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse time estimate\n",
    "minutes = []\n",
    "with open(\"../data/sword2/SWORD2/misc/new_iid/estimate_time.txt\") as f:\n",
    "\tlines = f.readlines()\n",
    "\tfor i in range(0, len(lines), 3):\n",
    "\t\t_, mins, id = lines[i], lines[i+1], lines[i+2]\n",
    "\t\tmins = mins.split('    ')\n",
    "\t\tmins = int(mins[1].strip())\n",
    "\t\tid = id.split('    ')\n",
    "\t\tid = id[1].strip()\n",
    "\t\tminutes.append((mins, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_minutes = [x[0] for x in minutes]\n",
    "np.mean(just_minutes)\n",
    "len(just_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1shs', 'P19632'),\n",
       " ('1shs', 'P61972'),\n",
       " ('1shs', 'P68699'),\n",
       " ('1shs', 'P45600'),\n",
       " ('1shs', 'P37064'),\n",
       " ('1shs', 'Q05506'),\n",
       " ('1shs', 'P33447'),\n",
       " ('1shs', 'P00968'),\n",
       " ('1shs', 'Q9UVT8'),\n",
       " ('1shs', 'P59936'),\n",
       " ('1shs', 'P38636'),\n",
       " ('1shs', 'P10618'),\n",
       " ('1shs', 'Q08602'),\n",
       " ('1shs', 'Q08603'),\n",
       " ('1shs', 'Q9REC4'),\n",
       " ('1shs', 'Q56308'),\n",
       " ('1shs', 'P28298'),\n",
       " ('1shs', 'Q05581'),\n",
       " ('1shs', 'P95474'),\n",
       " ('1shs', 'P31101'),\n",
       " ('1shs', 'Q9ZBA9'),\n",
       " ('1shs', 'O76463'),\n",
       " ('1shs', 'P80176'),\n",
       " ('1shs', 'Q56694'),\n",
       " ('1shs', 'Q50744'),\n",
       " ('1shs', 'P06149'),\n",
       " ('1shs', 'P0ABJ1'),\n",
       " ('1shs', 'P29208'),\n",
       " ('1shs', 'Q8ZYK1'),\n",
       " ('1shs', 'P72322'),\n",
       " ('1shs', 'P44853'),\n",
       " ('1shs', 'P20356'),\n",
       " ('1shs', 'P96142'),\n",
       " ('1shs', 'P03012'),\n",
       " ('1shs', 'Q9WY48'),\n",
       " ('1shs', 'Q57840'),\n",
       " ('1shs', 'P81156'),\n",
       " ('1shs', 'P62993'),\n",
       " ('1shs', 'P04425'),\n",
       " ('1shs', 'P9WQB5')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_pairs[:40]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWORD parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sword2(code, version, verb=False):\n",
    "    file = f\"../data/sword2/SWORD2/results/{version}/{code}/{code}_A/sword.txt\"\n",
    "    with open(file, \"r\") as f:\n",
    "        data = {}\n",
    "        lines = f.readlines()\n",
    "        option = 0\n",
    "        for i, line in enumerate(lines):\n",
    "            lines[i] = \"\".join([c for c in line if c not in [\"\\n\",'']])\n",
    "            if line != \"\\n\":\n",
    "                if not line.startswith((\"PDB:\", \"#D\", \"A\")):\n",
    "                    res = lines[i].split(\"|\")\n",
    "                    boundaries = res[2]\n",
    "                    domains = boundaries.strip().split(\" \")\n",
    "                    data[f\"option{option}\"] = {}\n",
    "                    for j in range(len(domains)):\n",
    "                        data[f\"option{option}\"][str(j+1)] = domains[j]\n",
    "                    option += 1\n",
    "    verb and pprint(data)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_sim(seq1, seq2, match_score = 1, mismatch_score = -1, gap_penalty = -2):\n",
    "\talignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "\n",
    "\t# Print the alignment(s)\n",
    "\t# for alignment in alignments:\n",
    "\t\t# print(format_alignment(*alignment))\n",
    "\tscore = alignments[0].score\n",
    "\tnorm_score = score / max(len(seq1), len(seq2))\n",
    "\treturn norm_score\n",
    "\n",
    "\n",
    "\n",
    "def dbd_score(y_pred, y_true, margin=20):\n",
    "    scores = []\n",
    "    for i in range(len(y_pred)):\n",
    "        window = y_true[max(0, i-margin):min(len(y_true), i+margin+1)]\n",
    "        indices_window = list(range(max(0, i-margin), min(len(y_true), i+margin+1)))\n",
    "        if y_pred[i] == 1.0:\n",
    "            if 1.0 in window:\n",
    "                # if it's within the window, calculate the score\n",
    "                pos = np.where(window == 1.0)[0][0]\n",
    "                j = indices_window[pos]\n",
    "                diff = abs(i - j)\n",
    "                k = 0 if diff == 0 else 1\n",
    "                score = ((margin - diff) + k) / margin\n",
    "            else:\n",
    "                # false positive\n",
    "                score = 0\n",
    "            scores.append(score)\n",
    "\n",
    "    number_of_true_boundaries = np.sum(y_true)\n",
    "    number_of_pred_boundaries = np.sum(y_pred)\n",
    "    max_len = max(number_of_true_boundaries,number_of_pred_boundaries)\n",
    "    if max_len == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return np.sum(scores) / max_len\n",
    "\n",
    "\n",
    "def observations(y_pred, y_true, margin):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    dbd = dbd_score(y_pred, y_true, margin)\n",
    "    for i in range(len(y_pred)):\n",
    "        window = y_true[max(0, i-margin):min(len(y_true), i+margin+1)]\n",
    "        indices_window = list(range(max(0, i-margin), min(len(y_true), i+margin+1)))\n",
    "        if y_pred[i] == 1.0:\n",
    "            if 1.0 in window:\n",
    "                pos = np.where(window == 1.0)[0][0]\n",
    "                j = indices_window[pos]\n",
    "                y_true[j] = 0.0\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "\n",
    "        elif y_pred[i] == 0.0:\n",
    "            if  y_true[i] == 1.0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def observations__(y_pred, y_true, margin):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    dbd = dbd_score(y_pred, y_true, margin)\n",
    "    for i in range(len(y_pred)):\n",
    "        window = y_true[max(0, i-margin):min(len(y_true), i+margin+1)]\n",
    "        indices_window = list(range(max(0, i-margin), min(len(y_true), i+margin+1)))\n",
    "        if y_pred[i] == 1.0:\n",
    "            if 1.0 in window:\n",
    "                pos = np.where(window == 1.0)[0][0]\n",
    "                j = indices_window[pos]\n",
    "                y_true[j] = 0.0\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 0.0:\n",
    "            if  y_true[i] == 1.0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    return (tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def metrics(y_pred, y_true, margin=20):\n",
    "    tp, tn, fp, fn = observations__(y_pred, y_true, margin)\n",
    "\n",
    "    accuracy = (tn + tp) / (tn + tp + fn + fp) if (tn + tp + fn + fp) else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    mcc_num = (tp * tn) - (fp * fn)\n",
    "    mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = mcc_num / mcc_den if mcc_den else 0\n",
    "\n",
    "    dbd = dbd_score(y_pred, y_true, margin)\n",
    "\n",
    "    return (accuracy, precision, recall, f1, mcc, dbd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(retries=3,\n",
    "                           backoff_factor=0.3,\n",
    "                           status_forcelist=(500, 502, 504),\n",
    "                           session=None):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def download_af_model(id):\n",
    "    \"\"\"\n",
    "    Download the Alphafold2 model corresponding to the Uniprot Id given by user\n",
    "    https://alphafold.ebi.ac.uk/\n",
    "\n",
    "    Returns:\n",
    "        - File path (string): Path of the downloaded PDB file\n",
    "        or\n",
    "        False if wrong id\n",
    "        \"DOWNLOAD ERROR\" if could not download\n",
    "    \"\"\"\n",
    "    RESULTS_DIR = \"../data/sword2/SWORD2/misc/new_iid/af_pdbs/\"\n",
    "    name = f\"AF-{id}-F1-model_v3\"\n",
    "    url = f\"https://alphafold.ebi.ac.uk/files/{name}.pdb\"\n",
    "    try:\n",
    "        response = requests_retry_session().get(url)\n",
    "    except Exception as x:\n",
    "        return (False, x)\n",
    "    with open(f\"{RESULTS_DIR}/{name}.pdb\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    return (True, f\"{RESULTS_DIR}/{name}.pdb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundaries2(len_seq, domain, discontinuity_delimiter):\n",
    "\t\"\"\"\n",
    "\t\tDefines a boundary as the beginning of a domain ONLY in multi-domain proteins\n",
    "\t\"\"\"\n",
    "\tfirst_start = np.inf\n",
    "\tbounds = np.zeros((len_seq), dtype=np.int8)\n",
    "\tfor k, v in domain.items():\n",
    "\t\tboundary_positions = v.split(discontinuity_delimiter)\n",
    "\t\tfor b in boundary_positions:\n",
    "\t\t\tstart, end = [int(i) for i in b.split('-')]\n",
    "\t\t\tif start < first_start:\n",
    "\t\t\t\tfirst_start = start\n",
    "\t\t\tbounds[start-1] = 1\n",
    "\tbounds[first_start-1] = 0            \n",
    "\treturn np.array(bounds, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_af_chain(code, chain):\n",
    "\tfile_path = f\"../data/sword2/SWORD2/misc/new_iid/af_pdbs/AF-{code}-F1-model_v3.pdb\"\n",
    "\tchains = {record.id: record.seq for record in SeqIO.parse(file_path, 'pdb-seqres')}\n",
    "\tseq = chains.get(f'XXXX:{chain}')\n",
    "\treturn seq\n",
    "\n",
    "\n",
    "def get_pdb_chain(code, chain):\n",
    "\tpdb_file_path = f\"../data/pdb/new_iid/{code}.pdb\"\n",
    "\tpdb_chains = {record.id: record.seq for record in SeqIO.parse(pdb_file_path, 'pdb-seqres')}\n",
    "\n",
    "\tfor key in pdb_chains.keys():\n",
    "\t\tif key[-1] == 'A':\n",
    "\t\t\ta_chain_pdb_seq = pdb_chains[key]\n",
    "\t\t\treturn a_chain_pdb_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6882472016116853"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypr = np.array([0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0], dtype=np.float16)\n",
    "ytr = np.array([0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0], dtype=np.float16)\n",
    "\n",
    "mcc = metrics(ypr, ytr)[-2]\n",
    "mcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/443]\n",
      "[100/443]\n",
      "[150/443]\n",
      "[200/443]\n",
      "[250/443]\n",
      "File not found Q9YDZ4 2hls\n",
      "[300/443]\n",
      "[350/443]\n",
      "File not found P38505 1jfj\n",
      "[400/443]\n",
      "[443/443]\n"
     ]
    }
   ],
   "source": [
    "path = '../data/sword2/final_results/results/af'\n",
    "\n",
    "uniprots = os.listdir(path)\n",
    "# print(uniprots)\n",
    "# print()\n",
    "\n",
    "pdb_mcc = []\n",
    "af_mcc = []\n",
    "\n",
    "pdb_f1 = []\n",
    "af_f1 = []\n",
    "\n",
    "pdb_no_bounds = []\n",
    "af_no_bounds = []\n",
    "\n",
    "for i, id in enumerate(uniprots):\n",
    "\n",
    "    pdb = pdb_uniprot_mappings_reverse[id]\n",
    "    a_chain_uniprot_seq = get_af_chain(id)\n",
    "    a_chain_pdb_seq = get_pdb_chain(pdb)\n",
    "    chain_len = None\n",
    "    if len(a_chain_pdb_seq) != len(a_chain_uniprot_seq):\n",
    "        raise ValueError(\"Different sequence lengths is not expected\")\n",
    "    else:\n",
    "        chain_len = len(a_chain_pdb_seq)\n",
    "    \n",
    "    baseline = cath[pdb]['A']\n",
    "    af_sword_results = get_sword2(id, 'af', verb=False)\n",
    "    try:\n",
    "        pdb_sword_results = get_sword2(pdb, 'pdb', verb=False)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\", id, pdb)\n",
    "        continue\n",
    "\n",
    "    margin = 20\n",
    "    baseline_boundaries = boundaries2(len(a_chain_pdb_seq), baseline, ',').astype(int)\n",
    "    pdb_mccs = []\n",
    "    af_mccs = []\n",
    "    pdb_dbds = []\n",
    "    af_dbds = []\n",
    "    pdb_f1s = []\n",
    "    af_f1s = []\n",
    "    pdb_bounds = []\n",
    "    af_bounds = []\n",
    "\n",
    "    for option, domain in pdb_sword_results.items():\n",
    "        pdb_sword_boundaries = boundaries2(chain_len, domain, ';').astype(int)\n",
    "        pdb_sword_metrics = metrics(pdb_sword_boundaries, baseline_boundaries, margin)\n",
    "        pdb_sword_mcc = pdb_sword_metrics[-2]\n",
    "        pdb_sword_dbd = pdb_sword_metrics[-1]\n",
    "        pdb_sword_f1 = pdb_sword_metrics[-3]\n",
    "        pdb_mccs.append(pdb_sword_mcc)\n",
    "        pdb_dbds.append(pdb_sword_dbd)\n",
    "        pdb_f1s.append(pdb_sword_f1)\n",
    "        pdb_bounds.append(np.sum(pdb_sword_boundaries))\n",
    "\n",
    "    for option, domain in af_sword_results.items():\n",
    "        af_sword_boundaries = boundaries2(chain_len, domain, ';').astype(int)\n",
    "        af_sword_metrics = metrics(af_sword_boundaries, baseline_boundaries, margin)\n",
    "        af_sword_mcc = af_sword_metrics[-2]\n",
    "        af_sword_dbd = af_sword_metrics[-1]\n",
    "        af_sword_f1 = af_sword_metrics[-3]\n",
    "        af_mccs.append(af_sword_mcc)\n",
    "        af_dbds.append(af_sword_dbd)\n",
    "        af_f1s.append(af_sword_f1)\n",
    "        af_bounds.append(np.sum(af_sword_boundaries))\n",
    "\n",
    "    af_f1.append(max(af_f1s))\n",
    "    pdb_f1.append(max(pdb_f1s))\n",
    "\n",
    "    best_pdb_i = np.argmax(pdb_mccs)\n",
    "    best_af_i = np.argmax(af_mccs)\n",
    "    pdb_no_bounds.append(pdb_bounds[best_pdb_i])\n",
    "\n",
    "    pdb_mcc.append(pdb_mccs[best_pdb_i])\n",
    "    af_mcc.append(af_mccs[best_af_i])\n",
    "    af_no_bounds.append(af_bounds[best_af_i])\n",
    "\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"[{i + 1}/{len(uniprots)}]\")\n",
    "\n",
    "print(f\"[{i + 1}/{len(uniprots)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af 2.2448979591836733\n",
      "pdb 2.941043083900227\n"
     ]
    }
   ],
   "source": [
    "print(\"af\", np.mean(af_no_bounds))\n",
    "print(\"pdb\", np.mean(pdb_no_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean PDB MCC: 0.6210332441611623\n",
      "Mean AF MCC: 0.13217732495763904\n",
      "27.3760386666944 7.451803918104624e-120\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = ttest_ind(pdb_mcc, af_mcc)\n",
    "\n",
    "print(\"Mean PDB MCC:\", np.mean(pdb_mcc))\n",
    "print(\"Mean AF MCC:\", np.mean(af_mcc))\n",
    "\n",
    "print(t_stat, float(p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean PDB MCC: 0.6740580770787745\n",
      "Mean AF MCC: 0.1436396457669393\n",
      "28.077329120492674 2.2876669819673796e-124\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = ttest_ind(pdb_mcc, af_mcc)\n",
    "\n",
    "print(\"Mean PDB MCC:\", np.mean(pdb_mcc))\n",
    "print(\"Mean AF MCC:\", np.mean(af_mcc))\n",
    "\n",
    "print(t_stat, float(p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean PDB MCC: 2.9115646258503403\n",
      "Mean AF MCC: 2.240362811791383\n",
      "-5.973230598758096 3.373535054941592e-09\n"
     ]
    }
   ],
   "source": [
    "t_stat, p_val = ttest_ind(af_no_bounds, pdb_no_bounds)\n",
    "\n",
    "print(\"Mean PDB MCC:\", np.mean(pdb_no_bounds))\n",
    "print(\"Mean AF MCC:\", np.mean(af_no_bounds))\n",
    "\n",
    "print(t_stat, float(p_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean PDB MCC: 0.7312201785001735\n",
    "\n",
    "Mean AF MCC: 0.04587217486107879\n",
    "\n",
    "56.20185031552613 2.0480532531702836e-293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "\n",
    "# def sensitivity(y_true, y_pred):\n",
    "#     true_label = K.argmax(y_true, axis=-1)\n",
    "#     pred_label = K.argmax(y_pred, axis=-1)\n",
    "#     INTERESTING_CLASS_ID = 2\n",
    "#     sample_mask = K.cast(K.not_equal(true_label, INTERESTING_CLASS_ID), 'int32')\n",
    "\n",
    "#     TP_tmp1 = K.cast(K.equal(true_label, 0), 'int32') * sample_mask\n",
    "#     TP_tmp2 = K.cast(K.equal(pred_label, 0), 'int32') * sample_mask    \n",
    "#     TP = K.sum(TP_tmp1 * TP_tmp2)\n",
    "\n",
    "#     FN_tmp1 = K.cast(K.equal(true_label, 0), 'int32') * sample_mask\n",
    "#     FN_tmp2 = K.cast(K.not_equal(pred_label, 0), 'int32') * sample_mask    \n",
    "#     FN = K.sum(FN_tmp1 * FN_tmp2)\n",
    "\n",
    "#     epsilon = 0.000000001\n",
    "#     return K.cast(TP, 'float') / (K.cast(TP, 'float') + K.cast(FN, 'float') + epsilon)\n",
    "\n",
    "\n",
    "# def precision(y_true, y_pred):\n",
    "#     true_label = K.argmax(y_true, axis=-1)\n",
    "#     pred_label = K.argmax(y_pred, axis=-1)\n",
    "#     INTERESTING_CLASS_ID = 2\n",
    "#     sample_mask = K.cast(K.not_equal(true_label, INTERESTING_CLASS_ID), 'int32')\n",
    "\n",
    "#     TP_tmp1 = K.cast(K.equal(true_label, 0), 'int32') * sample_mask\n",
    "#     TP_tmp2 = K.cast(K.equal(pred_label, 0), 'int32') * sample_mask\n",
    "#     TP = K.sum(TP_tmp1 * TP_tmp2)\n",
    "\n",
    "#     FP_tmp1 = K.cast(K.not_equal(true_label, 0), 'int32') * sample_mask\n",
    "#     FP_tmp2 = K.cast(K.equal(pred_label, 0), 'int32') * sample_mask\n",
    "#     FP = K.sum(FP_tmp1 * FP_tmp2)\n",
    "\n",
    "#     epsilon = 0.000000001\n",
    "#     return K.cast(TP, 'float') / (K.cast(TP, 'float') + K.cast(FP, 'float') + epsilon)\n",
    "\n",
    "# def f1_score(y_true, y_pred):\n",
    "#     pre = precision(y_true, y_pred)\n",
    "#     sen = sensitivity(y_true, y_pred)\n",
    "#     epsilon = 0.000000001\n",
    "#     f1 = 2 * pre * sen / (pre + sen + epsilon)\n",
    "#     return f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b6993604730f8cf29f5900b73e8b88206f8a6983047fdc957f382b3e5d49baa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
